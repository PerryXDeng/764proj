{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dce1b287-2f50-4c4b-8454-b807568dd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib widget\n",
    "from mise import MISE\n",
    "from networks.networks_lgan import Generator, Discriminator\n",
    "from networks.networks_seq2seq import Seq2SeqAE\n",
    "from networks.networks_partae import PartImNetAE\n",
    "from dataset.data_utils import n_parts_map\n",
    "from util.visualization import partsdf2mesh, partsdf2voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b3ad1da-8c4b-472b-afe7-72112c2a56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load networks\n",
    "gan_checkpoint = torch.load(os.path.join(\"chkt_dir\", \"lgan\", \"ckpt_epoch90000.pth\"))\n",
    "n_dim = 128 # dimension of noise vector'\n",
    "h_dim = 2048 # dimension of MLP hidden layer\n",
    "z_dim = 1024 # dimension of shape code, why is it different from chair latent dim, hidden_size?\n",
    "netG = Generator(n_dim, h_dim, z_dim).cuda()\n",
    "netG.load_state_dict(gan_checkpoint['netG_state_dict'])\n",
    "netD = Discriminator(h_dim, z_dim).cuda()\n",
    "netD.load_state_dict(gan_checkpoint['netD_state_dict'])\n",
    "\n",
    "max_n_parts = 9\n",
    "en_z_dim = 128 # part latent dim\n",
    "hidden_size = 256 # chair latent dim\n",
    "boxparam_size = 6 # dimension for part box parameters\n",
    "part_feat_size = en_z_dim + boxparam_size\n",
    "en_input_size = part_feat_size + n_parts_map(max_n_parts) + 1 # seq2seq input size\n",
    "de_input_size = part_feat_size\n",
    "n_layer = 2\n",
    "max_length = 10 # max seq length\n",
    "netSeq2Seq = Seq2SeqAE(en_input_size, de_input_size, hidden_size)\n",
    "seq2seq_checkpoint = torch.load(os.path.join(\"chkt_dir\", \"seq2seq\", \"ckpt_epoch2000.pth\"))\n",
    "netSeq2Seq.load_state_dict(seq2seq_checkpoint[\"model_state_dict\"])\n",
    "netDecoder = netSeq2Seq.decoder.cuda()\n",
    "del netSeq2Seq\n",
    "en_n_layers = 5\n",
    "en_f_dim = 32\n",
    "de_n_layers = 6\n",
    "de_f_dim = 128\n",
    "imnet_checkpoint = torch.load(os.path.join('chkt_dir', 'partae', 'latest.pth'))\n",
    "part_imnet = PartImNetAE(en_n_layers, en_f_dim, de_n_layers, de_f_dim, en_z_dim)\n",
    "part_imnet.load_state_dict(imnet_checkpoint['model_state_dict'])\n",
    "part_encoder = part_imnet.encoder.cuda()\n",
    "part_decoder = part_imnet.decoder.cuda()\n",
    "\n",
    "nets = [netG, netD, netDecoder, part_decoder]\n",
    "for n in nets:\n",
    "    for p in n.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f7292dc-34fc-4c4f-af2f-861e9c365358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_decoder(decoder, decoder_hidden, length=None): # outputs for a series of parts\n",
    "    decoder_outputs = []\n",
    "    decoder_input = decoder.init_input.detach().repeat(1, 1, 1).cuda()\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, output_seq, stop_sign = decoder(decoder_input, decoder_hidden)\n",
    "        decoder_outputs.append(output_seq)\n",
    "        if length is not None:\n",
    "            if di == length - 1:\n",
    "                break\n",
    "        elif torch.sigmoid(stop_sign[0, 0]) > 0.5:\n",
    "            # stop condition\n",
    "            break\n",
    "        decoder_input = output_seq.detach().unsqueeze(0)  # using target seq as input\n",
    "    decoder_outputs = torch.stack(decoder_outputs, dim=0)\n",
    "    return {\"boxparams\":decoder_outputs[:, :, -boxparam_size:], \"vecs\":decoder_outputs[:,:,:-boxparam_size]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b204c4-aede-4769-bcfc-2cd6dfa478bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_part_sdf(self, part_idx):\n",
    "    \"\"\"get output part sdf\n",
    "\n",
    "    :param part_idx: int\n",
    "    :return: all_points: (n_points, 3)\n",
    "             all_values: (n_points, )\n",
    "    \"\"\"\n",
    "    mesh_extractor = MISE(self.vox_dim, self.upsampling_steps, self.threshold)\n",
    "\n",
    "    points = mesh_extractor.query()\n",
    "\n",
    "    while points.shape[0] != 0:\n",
    "        # Query points\n",
    "        pointsf = torch.FloatTensor(points).cuda()\n",
    "        # rescale to range (0, 1)\n",
    "        pointsf = pointsf / mesh_extractor.resolution\n",
    "\n",
    "        values = self.eval_part_points(pointsf, part_idx).astype(np.double)\n",
    "\n",
    "        mesh_extractor.update(points, values)\n",
    "        points = mesh_extractor.query()\n",
    "\n",
    "    all_points, all_values = mesh_extractor.get_points()\n",
    "    return all_points, all_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "70d9de5e-5a24-4316-baa4-75454be2b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_dim = 64\n",
    "threshold = 0.5 # for isosurface reconstruction\n",
    "upsampling_steps = 0 # result resolution = 64 * 2^steps\n",
    "resolution = vox_dim * (1 << upsampling_steps)\n",
    "points_batch_size = 16*16*16*4\n",
    "bypart = True\n",
    "\n",
    "\n",
    "def infer_part_decoder(part_codes, points):\n",
    "    \"\"\"run part ae decoder to calculate part sdf\n",
    "\n",
    "    :param part_codes: (n_parts, 1, en_z_dim)\n",
    "    :param points: (n_parts, n_points, 3) value range (0, 1)\n",
    "    :return: out: ndarray (n_parts, n_points, 1) output sdf values for each point\n",
    "    \"\"\"\n",
    "    pred_n_parts = part_codes.shape[0]\n",
    "    if points.size(0) != pred_n_parts:\n",
    "        raise RuntimeError(\"pred:{} gt:{}\".format(pred_n_parts, points.size(0)))\n",
    "    n_points = points.size(1)\n",
    "\n",
    "    num = n_points // points_batch_size\n",
    "    if n_points % points_batch_size > 0:\n",
    "        num += 1\n",
    "    output_sdf = []\n",
    "    for i in range(num):\n",
    "        batch_points = points[:, i * points_batch_size:(i + 1) * points_batch_size, :]\n",
    "        cur_n_points = batch_points.size(1)\n",
    "        batch_z = part_codes.repeat((1, cur_n_points, 1)).view(-1, part_codes.size(-1))\n",
    "        batch_points = batch_points.contiguous().view(-1, 3)\n",
    "\n",
    "        out = part_decoder(batch_points, batch_z)\n",
    "        out = out.view((pred_n_parts, cur_n_points, -1))\n",
    "        out = out.detach().cpu().numpy()\n",
    "        output_sdf.append(out)\n",
    "    output_sdf = np.concatenate(output_sdf, axis=1)\n",
    "    return output_sdf\n",
    "\n",
    "\n",
    "def eval_part_points(points, part_vec):\n",
    "    values = infer_part_decoder(part_vec, points.unsqueeze(0))\n",
    "    return values.squeeze()\n",
    "\n",
    "def eval_part_sdf(part_vec):\n",
    "    \"\"\"get output part sdf\n",
    "\n",
    "    :param part_vec: [1, z_dim]\n",
    "    :return: all_points: (n_points, 3)\n",
    "             all_values: (n_points, )\n",
    "    \"\"\"\n",
    "    mesh_extractor = MISE(vox_dim, upsampling_steps, threshold)\n",
    "\n",
    "    points = mesh_extractor.query()\n",
    "\n",
    "    while points.shape[0] != 0:\n",
    "        # Query points\n",
    "        pointsf = torch.FloatTensor(points).cuda()\n",
    "        # rescale to range (0, 1)\n",
    "        pointsf = pointsf / mesh_extractor.resolution\n",
    "\n",
    "        values = eval_part_points(pointsf, part_vec).astype(np.double)\n",
    "\n",
    "        mesh_extractor.update(points, values)\n",
    "        points = mesh_extractor.query()\n",
    "\n",
    "    all_points, all_values = mesh_extractor.get_points()\n",
    "    return all_points, all_values\n",
    "\n",
    "def transform_points(points, values, transforms):\n",
    "    \"\"\"transform part points from local frame to global frame\n",
    "\n",
    "    :transforms: [n_parts, 1, 6] same thing as boxparams\n",
    "    :param points: (n_parts, n_points, 3) or [(n_points1, 3), (n_points2, 3), ...], in range (0, vox_dim)\n",
    "    :param values: (n_parts, n_points, 1) or [(n_points1, 1), (n_points2, 1), ...]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cube_mid = np.asarray([resolution // 2, resolution // 2, resolution // 2]).reshape(1, 3)\n",
    "    new_points, new_values = [], []\n",
    "    for idx in range(len(points)):\n",
    "        part_points = points[idx]\n",
    "        part_values = values[idx]\n",
    "        part_translation = (transforms[idx, 0, :3].reshape(1, 3) * resolution).detach().cpu().numpy()\n",
    "        part_size = transforms[idx, 0, 3:6].reshape(3).detach().cpu().numpy()\n",
    "\n",
    "        part_scale = np.amax(part_size)\n",
    "        part_points = (part_points - cube_mid) * part_scale + part_translation\n",
    "\n",
    "        mins = part_translation - part_size * resolution / 2\n",
    "        maxs = part_translation + part_size * resolution / 2\n",
    "        in_bbox_indice = np.max(part_points - maxs, axis=1)\n",
    "        in_bbox_indice = np.where(in_bbox_indice <= 0)[0]\n",
    "        part_points = part_points[in_bbox_indice, :]\n",
    "        part_values = part_values[in_bbox_indice]\n",
    "\n",
    "        in_bbox_indice = np.max(part_points - mins, axis=1)\n",
    "        in_bbox_indice = np.where(in_bbox_indice >= 0)[0]\n",
    "        part_points = part_points[in_bbox_indice, :]\n",
    "        part_values = part_values[in_bbox_indice]\n",
    "\n",
    "        part_points = np.clip(part_points, 0, resolution - 1)\n",
    "        new_points.append(part_points)\n",
    "        new_values.append(part_values)\n",
    "    return new_points, new_values\n",
    "\n",
    "def generate_shape(part_vecs, transforms, by_part=True):\n",
    "    \"\"\"generate final shape geometry\n",
    "\n",
    "    :param part_vecs: [n_parts, 1, z_dim]\n",
    "    :param format: str. output geometry format\n",
    "    :param by_part: bool. segment each part or put as a whole\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    values = []\n",
    "    for idx in range(part_vecs.shape[0]):\n",
    "        part_points, part_values = eval_part_sdf(part_vecs[idx])\n",
    "        points.append(part_points)\n",
    "        values.append(part_values)\n",
    "    points, values = transform_points(points, values, transforms)\n",
    "    # shape_voxel = partsdf2voxel(points, values, vox_dim=resolution, by_part=by_part)\n",
    "    shape_mesh = partsdf2mesh(points, values, affine=None, vox_dim=resolution, by_part=by_part)\n",
    "    return {'mesh':shape_mesh}\n",
    "\n",
    "def save_output(shape, filename, save_dir, form):\n",
    "    if form == 'voxel':\n",
    "        save_path = os.path.join(save_dir, \"{}.h5\".format(filename))\n",
    "        with h5py.File(save_path, 'w') as fp:\n",
    "            fp.create_dataset('voxel', data=shape, compression=9)\n",
    "    elif form == \"mesh\":\n",
    "        save_path = os.path.join(save_dir, \"{}.obj\".format(filename))\n",
    "        shape.export(save_path)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "def decode_n_save_shape_mesh(part_vecs, transforms, save_dir, filename):\n",
    "    mesh = generate_shape(part_vecs, transforms)['mesh']\n",
    "    save_output(mesh, filename, save_dir, form=\"mesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcf41e64-d7db-4e88-a002-1fb5b00ff9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pqnet_generation():\n",
    "    noise = torch.randn(n_dim).cuda()\n",
    "    with torch.no_grad():\n",
    "        fake = netG(noise)\n",
    "        score = netD(fake)\n",
    "        seq = infer_decoder(netDecoder, fake.view(2, 1, hidden_size*2))\n",
    "        boxs = seq['boxparams'] # # [n_parts, 1, boxparam_dim]\n",
    "        vecs = seq['vecs'] # [n_parts, 1, en_z_dim]\n",
    "        decode_n_save_shape_mesh(vecs, boxs, os.path.join('test','meshes'), 'model_pqnet_gan')\n",
    "\n",
    "test_pqnet_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "914aa9a3-5a3c-4b33-bf25-d1f5d7145bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data utilities\n",
    "import h5py\n",
    "\n",
    "json_dir = \"data/parts_json\"\n",
    "\n",
    "def loadH5Full(path, resolution=64, rescale=True):\n",
    "    with h5py.File(path, 'r') as data_dict:\n",
    "        nParts = data_dict.attrs['n_parts']\n",
    "        partVoxel = data_dict['parts_voxel_scaled64'][:].astype(np.float)\n",
    "        dataPoints = data_dict['points_{}'.format(resolution)][:]\n",
    "        dataVals = data_dict['values_{}'.format(resolution)][:]\n",
    "        translation = data_dict['translations'][:]\n",
    "        scale = data_dict['scales'][:]\n",
    "        size = data_dict['size'][:]\n",
    "    if rescale:\n",
    "        dataPoints = dataPoints / resolution\n",
    "    return nParts, partVoxel, dataPoints, dataVals, scale, translation, size\n",
    "\n",
    "def readJsonPartCategories(chairID, nParts):\n",
    "    categories = []\n",
    "    path = os.path.join(json_dir, chairID, \"result.json\")\n",
    "    try:\n",
    "        f = open(path)\n",
    "        data = json.load(f)\n",
    "        for (i, el) in enumerate(data[0]['children']):\n",
    "            partName = el['text']\n",
    "            if partName == \"Chair Back\":\n",
    "                categories.append(1)\n",
    "            elif partName == \"Chair Seat\":\n",
    "                categories.append(2)\n",
    "            elif partName == \"Chair Arm\":\n",
    "                categories.append(3)\n",
    "            elif partName == \"Chair Base\":\n",
    "                # this will also apply to following parts\n",
    "                for j in range(i, nParts):\n",
    "                    categories.append(4)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        categories = [0]*nParts # missing corresponding json labels for input h5\n",
    "    return categories[0:nParts] # sometimes json file has more labels than voxelized parts, in which case truncated\n",
    "\n",
    "def getChairPartInfos(encoder, data_dir, filename):\n",
    "    path = os.path.join(data_dir, filename)\n",
    "\n",
    "    nParts, partVoxel, dataPoints, dataVals, scales, translations, size = loadH5Full(path, resolution=64)\n",
    "    voxelTensor = torch.tensor(partVoxel.astype(np.float), dtype=torch.float32).unsqueeze(1)  # (nParts, 1, dim, dim, dim)\n",
    "    with torch.no_grad():\n",
    "        latentVecs = encoder(voxelTensor.cuda()).cpu().numpy()\n",
    "    nParts = latentVecs.shape[0]\n",
    "    strid = filename[0:-3]\n",
    "    categories = np.array(readJsonPartCategories(strid, nParts))\n",
    "\n",
    "    # numpy arrays\n",
    "    # vecs: (nParts, latent dimension)\n",
    "    # scales: (nParts, 1)\n",
    "    # translations: (nParts, 3)\n",
    "    # categories: (nParts)\n",
    "    return {'vecs':latentVecs, 'scales':scales, 'translations':translations, 'categories':categories,\n",
    "            'filenames':[filename]*len(latentVecs)}\n",
    "\n",
    "def loadAllChairsInfoIterable(data_dir):\n",
    "    # part latent vectors, categories, and affine transforms\n",
    "    filenames = filter(lambda filename:filename.endswith(\".h5\"), os.listdir(data_dir))    \n",
    "    chairInfos = filter(lambda x:x is not None, map(lambda filename: getChairPartInfos(part_encoder, data_dir, filename), filenames))\n",
    "    \n",
    "    return chairInfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aa6d93a3-e7af-49f7-8d26-2d62073e9309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name: model_gana_0\n",
      "Critic score: tensor([1.0693])\n",
      "Using parts from: ['39055.h5' '39055.h5' '40096.h5' '40096.h5' '39055.h5' '37529.h5']\n",
      "\n",
      "Name: model_gana_1\n",
      "Critic score: tensor([-0.2922])\n",
      "Using parts from: ['37529.h5' '39055.h5' '37529.h5']\n",
      "\n",
      "Name: model_gana_2\n",
      "Critic score: tensor([-0.2619])\n",
      "Using parts from: ['39055.h5' '41975.h5' '37529.h5' '37529.h5' '37529.h5' '37529.h5']\n",
      "\n",
      "Name: model_gana_3\n",
      "Critic score: tensor([-0.3250])\n",
      "Using parts from: ['41975.h5' '41975.h5' '41975.h5' '41975.h5']\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "def load_targets_info(target_dir):\n",
    "    targets = list(loadAllChairsInfoIterable(target_dir))\n",
    "    part_originfiles = np.array(reduce(lambda l1, l2: l1+l2, map(lambda x:x['filenames'], targets)))\n",
    "    part_originindex = np.concatenate(list(map(lambda x:np.arange(0, len(x['filenames'])), targets)))\n",
    "    part_vecs = np.concatenate(list(map(lambda x:x['vecs'], targets)))\n",
    "    return {'vecs':part_vecs, 'originfiles':part_originfiles, 'originindex':part_originindex}\n",
    "\n",
    "\n",
    "save_dir = os.path.join('test','meshes')\n",
    "##########################\n",
    "# adversarial parameters #\n",
    "##########################\n",
    "targets_path = \"data/TestData/set2\"  # change this to new data as needed\n",
    "num_search = 4\n",
    "max_search_iter = 3000\n",
    "sample_size = 64\n",
    "learning_rate = 1e-4\n",
    "# uses weight decay to enforce gaussian prior for generator input\n",
    "# https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior\n",
    "weight_decay_param = 0\n",
    "\n",
    "#########################\n",
    "# adversarial algorithm #\n",
    "#########################\n",
    "targets = load_targets_info(targets_path)\n",
    "targetvecs = torch.cuda.FloatTensor(targets['vecs']).view(1, -1, en_z_dim) # [1, n_targetparts, en_z_dim]\n",
    "for kth_search in range(num_search):\n",
    "    adv_noise = torch.randn(n_dim, requires_grad=True, device=\"cuda\")  # N(0,1)\n",
    "    adv_adam = torch.optim.Adam([adv_noise], lr=learning_rate, betas=(0.5, 0.9), weight_decay=weight_decay_param)\n",
    "    distance_tracker = [None] * max_search_iter\n",
    "    scores_tracker = [None] * max_search_iter\n",
    "    means_tracker = [None] * max_search_iter\n",
    "    vars_tracker = [None] * max_search_iter\n",
    "    for ith_iter in range(max_search_iter):\n",
    "        fake = netG(adv_noise)\n",
    "        seq = infer_decoder(netDecoder, fake.view(2, 1, hidden_size*2))\n",
    "        vecs = seq['vecs'] # [n_parts, 1, en_z_dim]\n",
    "        distances = torch.norm(vecs - targetvecs, dim=2) # [n_parts, n_targetparts]\n",
    "        shortest_distances, closest_targets = torch.min(distances, dim=1) # [n_parts]\n",
    "        loss = torch.sum(shortest_distances)\n",
    "        loss.backward()\n",
    "        adv_adam.step()\n",
    "        adv_adam.zero_grad()\n",
    "        # for debugging\n",
    "        with torch.no_grad():\n",
    "            score = netD(fake)\n",
    "        distance_tracker[ith_iter] = loss.detach().to('cpu', non_blocking=True)\n",
    "        scores_tracker[ith_iter] = score.detach().to('cpu', non_blocking=True)\n",
    "        means_tracker[ith_iter] = torch.mean(adv_noise).detach().to('cpu', non_blocking=True)\n",
    "        vars_tracker[ith_iter] = torch.var(adv_noise, unbiased=True).detach().to('cpu', non_blocking=True)\n",
    "    \n",
    "    # visualizations & output\n",
    "    if num_search == 1: # plot debugging info when there is only one search\n",
    "        distance_tracker = np.array([distance.numpy() for distance in distance_tracker]) # [n_iterations]\n",
    "        scores_tracker = np.array([score.numpy() for score in scores_tracker]) # [n_iterations]\n",
    "        fig, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "        ax[0, 0].set_title(\"Total Distances over Time\")\n",
    "        ax[0, 0].plot(distance_tracker, marker='o')\n",
    "        ax[0, 0].set_xlabel(\"iteration\")\n",
    "        ax[0, 0].set_ylabel(\"total distance\")\n",
    "        ax[0, 1].set_title(\"Critic Score over Time\")\n",
    "        ax[0, 1].plot(scores_tracker, marker='o')\n",
    "        ax[0, 1].set_xlabel(\"iteration\")\n",
    "        ax[0, 1].set_ylabel(\"critic score\")\n",
    "        ax[1, 0].set_title(\"Input Means over Time\")\n",
    "        ax[1, 0].plot(means_tracker, marker='o')\n",
    "        ax[1, 0].set_xlabel(\"iteration\")\n",
    "        ax[1, 0].set_ylabel(\"mean\")\n",
    "        ax[1, 1].set_title(\"Input Variances over Time\")\n",
    "        ax[1, 1].plot(vars_tracker, marker='o')\n",
    "        ax[1, 1].set_xlabel(\"iteration\")\n",
    "        ax[1, 1].set_ylabel(\"variance\")\n",
    "    boxs = seq['boxparams'] # # [n_parts, 1, boxparam_dim]\n",
    "    # !!!!using closest parts in target range as part outputs!!!!\n",
    "    vecs = targetvecs[0, closest_targets, :].view(-1, 1, en_z_dim) # [n_parts, 1, en_z_dim]\n",
    "    out_name = f'model_gana_{kth_search}'\n",
    "    decode_n_save_shape_mesh(vecs, boxs, save_dir, out_name)\n",
    "    print()\n",
    "    print(\"Name:\", out_name)\n",
    "    print(\"Critic score:\", scores_tracker[-1])\n",
    "    print(\"Using parts from:\", targets['originfiles'][closest_targets.cpu().numpy()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
