{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dce1b287-2f50-4c4b-8454-b807568dd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from networks.networks_lgan import Generator, Discriminator\n",
    "from networks.networks_seq2seq import Seq2SeqAE\n",
    "from dataset.data_utils import n_parts_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b3ad1da-8c4b-472b-afe7-72112c2a56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_checkpoint = torch.load(os.path.join(\"chkt_dir\", \"lgan\", \"ckpt_epoch90000.pth\"))\n",
    "n_dim = 128 # dimension of noise vector'\n",
    "h_dim = 2048 # dimension of MLP hidden layer\n",
    "z_dim = 1024 # dimension of shape code, why is it different from chair latent dim, hidden_size?\n",
    "netG = Generator(n_dim, h_dim, z_dim).cuda()\n",
    "netG.load_state_dict(gan_checkpoint['netG_state_dict'])\n",
    "netD = Discriminator(h_dim, z_dim).cuda()\n",
    "netD.load_state_dict(gan_checkpoint['netD_state_dict'])\n",
    "\n",
    "max_n_parts = 9\n",
    "en_z_dim = 128 # part latent dim\n",
    "hidden_size = 256 # chair latent dim\n",
    "boxparam_size = 6 # dimension for part box parameters\n",
    "part_feat_size = en_z_dim + boxparam_size\n",
    "en_input_size = part_feat_size + n_parts_map(max_n_parts) + 1 # seq2seq input size\n",
    "de_input_size = part_feat_size\n",
    "n_layer = 2\n",
    "max_length = 10 # max seq length\n",
    "netSeq2Seq = Seq2SeqAE(en_input_size, de_input_size, hidden_size)\n",
    "seq2seq_checkpoint = torch.load(os.path.join(\"chkt_dir\", \"seq2seq\", \"ckpt_epoch2000.pth\"))\n",
    "netSeq2Seq.load_state_dict(seq2seq_checkpoint[\"model_state_dict\"])\n",
    "netDecoder = netSeq2Seq.decoder.cuda()\n",
    "def infer_decoder(decoder, decoder_hidden, length=None): # outputs for a series of parts\n",
    "    decoder_outputs = []\n",
    "    decoder_input = decoder.init_input.detach().repeat(1, 1, 1).cuda()\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, output_seq, stop_sign = decoder(decoder_input, decoder_hidden)\n",
    "        decoder_outputs.append(output_seq)\n",
    "        if length is not None:\n",
    "            if di == length - 1:\n",
    "                break\n",
    "        elif torch.sigmoid(stop_sign[0, 0]) > 0.5:\n",
    "            # stop condition\n",
    "            break\n",
    "        decoder_input = output_seq.detach().unsqueeze(0)  # using target seq as input\n",
    "    decoder_outputs = torch.stack(decoder_outputs, dim=0)\n",
    "    return {\"boxparams\":decoder_outputs[:, :, -boxparam_size:], \"vecs\":decoder_outputs[:,:,:-boxparam_size]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcf41e64-d7db-4e88-a002-1fb5b00ff9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_pqnet_generation():\n",
    "    noise = torch.randn(n_dim).cuda()\n",
    "    with torch.no_grad():\n",
    "        fake = netG(noise)\n",
    "        score = netD(fake)\n",
    "        seq = infer_decoder(netDecoder, fake.view(2, 1, 512))\n",
    "        boxs = seq['boxparams'] # # [n_parts, 1, 6]\n",
    "        vecs = seq['vecs'] # [n_parts, 1, 128]\n",
    "        print(seq['vecs'].shape)\n",
    "\n",
    "test_pqnet_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d93a3-e7af-49f7-8d26-2d62073e9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 64\n",
    "learning_rate = 1e-4\n",
    "\n",
    "adv_noise = torch.randn(sample_size, n_dim, requires_grad=True).cuda()  # N(0,1)\n",
    "# uses weight decay to enforce gaussian prior for generator input\n",
    "# https://stats.stackexchange.com/questions/163388/why-is-the-l2-regularization-equivalent-to-gaussian-prior\n",
    "weight_decay_param = 2e-1\n",
    "adv_adam = torch.optim.Adam([adv_noise], lr=learning_rate, betas=(0.5, 0.9))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
